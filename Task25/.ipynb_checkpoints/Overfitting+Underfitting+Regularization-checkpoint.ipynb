{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9973a6d1",
   "metadata": {},
   "source": [
    "# Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b4985c",
   "metadata": {},
   "source": [
    "Overfitting happens when a model learns the training data too well, to the point that it captures noise or irrelevant patterns that are specific to the training set but do not apply to new, unseen data. An overfit model tends to have high variance and performs poorly on test or validation data. It essentially \"memorizes\" the training examples instead of learning the general underlying patterns. Signs of overfitting include very low training error but high test error or poor performance on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f759e3",
   "metadata": {},
   "source": [
    "# Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395d9bf7",
   "metadata": {},
   "source": [
    "Underfitting occurs when a model is too simplistic and fails to capture the underlying patterns in the training data. An underfit model may have high bias and performs poorly both on the training data and new data. It essentially oversimplifies the problem and does not have enough capacity to learn the true relationship between the input features and the target variable. Signs of underfitting include both high training and test errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e8c8ac",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f4067a",
   "metadata": {},
   "source": [
    "Regularization is a technique used to address overfitting by adding a penalty term to the model's objective function. The penalty term discourages the model from learning overly complex or intricate patterns from the training data. It helps to prevent the model from relying too heavily on any particular feature or from fitting noise in the data. Regularization techniques commonly used include L1 regularization (Lasso), L2 regularization (Ridge), and Elastic Net, each with different ways of penalizing model complexity.\n",
    "\n",
    "Regularization can help mitigate underfitting to some extent, but it is primarily used to address overfitting. Underfitting occurs when a model is too simple and fails to capture the underlying patterns in the data. In such cases, regularization alone may not be sufficient to overcome underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dea93fc",
   "metadata": {},
   "source": [
    "# Movie Review Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52677b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "#vectorizing input data\n",
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "#print(x_train[0])\n",
    "\n",
    "#vectorizing labels\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19bcaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#originally\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7c98b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#smaller network\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(4, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa65ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigger network\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873c338e",
   "metadata": {},
   "source": [
    "# L1 regularization   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fe8161",
   "metadata": {},
   "source": [
    "The cost added is proportional to the absolute value of the weight coefficients (the L1 norm of the weights).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c20518",
   "metadata": {},
   "source": [
    "# L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d676cab",
   "metadata": {},
   "source": [
    "The cost added is proportional to the square of the value of the weight coefficients (the L2 norm of the weights). L2 regularization is also called weight decay in the context of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fba304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1232e20d",
   "metadata": {},
   "source": [
    "# DropOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee6841d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1473d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
