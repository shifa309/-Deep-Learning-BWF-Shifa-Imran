{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "436cd91a",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2004d16",
   "metadata": {},
   "source": [
    "Optimizers are algorithms used to adjust the parameters of a model in order to minimize the loss function during the training process. They determine how the model learns and updates its weights. Some commonly used optimizers include Stochastic Gradient Descent (SGD), Adam, RMSprop, and Adagrad. Each optimizer has its own update rule and hyperparameters.\n",
    "for example there is forward and backward propagation in neural nets where you update each of the weights and biases by finding partial derivatives.\n",
    "\n",
    "In case Gradient Descenet Algorithm:\n",
    "Weights and biases are updated using gradient descent algorithm:\n",
    "𝑤𝑖+1 = 𝑤𝑖 − 𝛼 𝜕𝐿/𝜕𝑤\n",
    "𝑏𝑖+1 = 𝑏𝑖 − 𝛼 𝜕𝐿/𝜕𝑏\n",
    "\n",
    "Propagation equations along with backpropagation algorithm is used to find the partial derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e81031",
   "metadata": {},
   "source": [
    "# Built-in Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d8f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define your model architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_dim, activation='softmax')\n",
    "])\n",
    "\n",
    "# Define your optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc6d6b9",
   "metadata": {},
   "source": [
    "Now in the code above, we can see \"Adam\" optimizer, relu activation function used in hidden layers and softmax function used in output as its multi class classification where \"categorical_crossentropy\" loss function is used.\n",
    "\n",
    "Lets delve in to activation and loss functions one by one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c00864b",
   "metadata": {},
   "source": [
    "# Activation Functions \n",
    "\n",
    "The last layer activation refers to the activation function applied to the output layer of a neural network. It is responsible for mapping the input to a suitable range or representation for the task at hand. The choice of activation function depends on the type of problem being solved. For regression tasks, linear activation or identity function is commonly used. For binary classification, sigmoid activation is often used. For multi-class classification, softmax activation is typically employed to produce a probability distribution over the classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298c1d65",
   "metadata": {},
   "source": [
    "# Sigmoid Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cffcba",
   "metadata": {},
   "source": [
    "Typically used in output layer for binary classification\n",
    "• Sigmoid Function:\n",
    "𝜎(𝑧) = 1/(1+𝑒^(−𝑧))\n",
    "\n",
    "Gives a value between zero and one\n",
    "Output can be interpreted as a probability of positive label\n",
    "Positive label if value greater than 0.5\n",
    "\n",
    "• Has vanishing gradient problem: It is a phenomenon that occurs in neural networks when using the sigmoid activation function (specifically the standard sigmoid or logistic sigmoid function). It refers to the issue where the gradients of the error function with respect to the weights and biases become extremely small as they are backpropagated to earlier layers of the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cdbeef",
   "metadata": {},
   "source": [
    "# Softmax Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269801ab",
   "metadata": {},
   "source": [
    "Used in output layer for multiclass classification problems\n",
    "Number of output neurons equal to the number of classes\n",
    "\n",
    "The sum of values of all output neurons is equal to 1\n",
    "The maximum value represents the predicted class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216f821",
   "metadata": {},
   "source": [
    "# ReLU Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a970a1e",
   "metadata": {},
   "source": [
    "Used in output layer for regression tasks\n",
    "Preferred choice in hidden layers\n",
    "Number of output neurons is equal to one\n",
    "\n",
    "• ReLU function:\n",
    "𝜎(𝑧) = max(0,𝑧)\n",
    "\n",
    "The output is either zero or a linear function\n",
    "The output represents the prediction\n",
    "• Might result in exploding gradient problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c178f57",
   "metadata": {},
   "source": [
    "# Loss functions\n",
    "\n",
    "Loss functions, also known as objective functions or cost functions, quantify the error between the predicted outputs of a model and the actual ground truth labels. They serve as a measure of how well the model is performing. The choice of loss function depends on the type of problem being solved. Mean Squared Error (MSE) is commonly used for regression tasks, while Binary Cross-Entropy and Categorical Cross-Entropy are frequently used for binary and multi-class classification tasks, respectively. There are also specialized loss functions for specific tasks such as ranking, sequence generation, and object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4c3b00",
   "metadata": {},
   "source": [
    "# Binary Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a5af26",
   "metadata": {},
   "source": [
    "Cross entropy is a measure of difference between two probability distributions.\n",
    "\n",
    "• Binary cross entropy loss:\n",
    "𝐿(𝑑,𝑧) = −[𝑑log(𝑧) + (1 − 𝑑)log(1−𝑧)]\n",
    "𝑑 is the actual output and 𝑧 is the prediction\n",
    "\n",
    "Used for binary classification problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8686f80",
   "metadata": {},
   "source": [
    "# Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0674097c",
   "metadata": {},
   "source": [
    "Also known as Categorical Cross Entropy\n",
    "Used for multiclass classification problems\n",
    "\n",
    "The cross entropy loss measures the dissimilarity between the true label and the predicted probability distribution. It encourages the predicted probabilities to align with the true label by penalizing large differences between the two. The loss is minimized when the predicted probabilities closely match the one-hot encoded true label.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6c35b6",
   "metadata": {},
   "source": [
    "# Mean Squared Error Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c87b49",
   "metadata": {},
   "source": [
    "Used for regression problems.\n",
    "\n",
    "• Mean squared error loss:\n",
    "L(d,z) = 1/2𝑁 ∑(𝑖=1 to 𝑁) (𝑧𝑖−𝑑𝑖)2\n",
    "\n",
    "𝑑 is the actual output and 𝑧 is the prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92681dd3",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4b687c",
   "metadata": {},
   "source": [
    "Evaluation metrics are used to measure the performance of a machine learning model. They provide quantitative measures that assess the accuracy, precision, recall, and other aspects of the model's predictions. The choice of evaluation metric depends on the specific problem and the desired outcome. Some commonly used evaluation metrics include accuracy, precision, recall, F1 score, area under the ROC curve (AUC-ROC), mean average precision (mAP), and mean squared error (MSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eb41d5",
   "metadata": {},
   "source": [
    "Commonly used evaluation metrics for binary classification tasks, along with their formulas in terms of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN):\n",
    "\n",
    "1. Accuracy:\n",
    "Accuracy measures the overall correctness of the model's predictions.\n",
    "Formula: Accuracy= TP+TN/TP+TN+FP+FN\n",
    "\n",
    "2. Precision:\n",
    "Precision measures the proportion of correctly predicted positive instances among all instances predicted as positive. It quantifies how precise the positive predictions are.\n",
    "Formula: Precision= TP/TP+FP\n",
    "\n",
    "3. Recall (Sensitivity or True Positive Rate):\n",
    "Recall measures the proportion of correctly predicted positive instances out of all actual positive instances. It quantifies how well the model identifies positive instances.\n",
    "Formula: Recall= TP/TP+FN\n",
    "\n",
    "4. F1 Score:\n",
    "The F1 score is the harmonic mean of precision and recall. It provides a balanced measure that combines both metrics.\n",
    "Formula:  F1 Score = 2×Precision×Recall/Precision+Recall\n",
    "\n",
    "Other evaluation metrics for Regression problems are:\n",
    "MSE, RMSE, MAE, R1 Score etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0421e11c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
