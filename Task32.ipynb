{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f2d1494",
   "metadata": {},
   "source": [
    "# Hybrid CNN-LSTM performing sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ea09eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 30s 37ms/step - loss: 0.3937 - accuracy: 0.8131 - val_loss: 0.3398 - val_accuracy: 0.8519\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 34s 44ms/step - loss: 0.2202 - accuracy: 0.9130 - val_loss: 0.3433 - val_accuracy: 0.8513\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.1090 - accuracy: 0.9626 - val_loss: 0.4448 - val_accuracy: 0.8322\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 0.4448 - accuracy: 0.8322\n",
      "Test Loss: 0.4448\n",
      "Test Accuracy: 0.8322\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Conv1D, MaxPooling1D, LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 10000  # Maximum number of words in the vocabulary\n",
    "max_len = 100  # Maximum length of input sequences\n",
    "embedding_dim = 100  # Dimensionality of word embeddings\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim, input_length=max_len))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=3, validation_data=(x_test, y_test))\n",
    "\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7927d8d",
   "metadata": {},
   "source": [
    "#  Hybrid CNN-LSTM network - performs image classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "216acb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 44s 55ms/step - loss: 1.3389 - accuracy: 0.5214 - val_loss: 1.1551 - val_accuracy: 0.5826\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 55s 70ms/step - loss: 0.9651 - accuracy: 0.6618 - val_loss: 0.9656 - val_accuracy: 0.6624\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.8071 - accuracy: 0.7186 - val_loss: 0.8694 - val_accuracy: 0.6989\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 0.6745 - accuracy: 0.7662 - val_loss: 0.8235 - val_accuracy: 0.7155\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.5539 - accuracy: 0.8098 - val_loss: 0.8351 - val_accuracy: 0.7184\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.4261 - accuracy: 0.8567 - val_loss: 0.8215 - val_accuracy: 0.7239\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 76s 97ms/step - loss: 0.3103 - accuracy: 0.9018 - val_loss: 0.8738 - val_accuracy: 0.7199\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.2057 - accuracy: 0.9403 - val_loss: 0.8850 - val_accuracy: 0.7287\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 59s 75ms/step - loss: 0.1258 - accuracy: 0.9707 - val_loss: 1.0126 - val_accuracy: 0.7151\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.0762 - accuracy: 0.9856 - val_loss: 1.0229 - val_accuracy: 0.7206\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.0229 - accuracy: 0.7206\n",
      "Test Loss: 1.0229\n",
      "Test Accuracy: 0.7206\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, LSTM, Reshape\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Reshape((1, -1)))  # Reshape the feature vector to match LSTM input\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ff2ce2",
   "metadata": {},
   "source": [
    "In the above code, CIFAR-10 dataset, which consists of 50,000 training images and 10,000 test images belonging to 10 different classes. The code builds a hybrid network with a CNN layer followed by an LSTM layer. The model takes as input RGB images, applies convolutional and pooling layers, flattens the output, and passes it to an LSTM layer. The final dense layer produces the classification output probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b6559",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
